{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('base': conda)",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# -*- coding: utf-8 -*-\n",
    "# Loading libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from networkx.algorithms.centrality import group\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "from swmmtoolbox import swmmtoolbox as swmm\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from concurrent import futures\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import scoped_session\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "import pyproj\n",
    "#%%\n",
    "\n",
    "model_id =  'model_' + input('4-digit model_id like: 0123:    ' )\n",
    "precipitation_id ='precipitation_' + input('4-digit raingage_id like 0123:    ')\n",
    "event_id = input('4-digit event_id like: 0123:    ')\n",
    "epsg_modelo = input('EPSG (ejemplo: 5348):    ')\n",
    "\n",
    "project_folder = os.path.abspath(os.path.join(os.getcwd(),\"..\"))\n",
    "data_raw_folder = os.path.join(project_folder,'data', 'raw')\n",
    "event_folder = os.path.join(data_raw_folder, 'Run_[' + event_id + ']')\n",
    "model_inp = os.path.join(event_folder, 'model.inp')\n",
    "model_out = os.path.join(event_folder, 'model.out')\n",
    "\n",
    "\n",
    "# Connection to database\n",
    "engine_base_ina = create_engine('postgresql://postgres:postgres@172.18.0.1:5555/base-ina')\n",
    "\n",
    "\n",
    "RELEVANT_GROUP_TYPES_OUT = [\n",
    "            'link',\n",
    "            'node',\n",
    "            'subcatchment',\n",
    "            # 'system'\n",
    "            ]\n",
    "\n",
    "RELEVANT_GROUP_TYPES_INP = [\n",
    "            'coordinates',\n",
    "            'subcatchments',\n",
    "            'raingages',\n",
    "            'conduits',\n",
    "            'orifices',\n",
    "            'weirs',\n",
    "            'outfalls',\n",
    "            # 'vertices',\n",
    "            # 'polygons',\n",
    "            'subareas',\n",
    "            # 'losses',\n",
    "            'infiltration',\n",
    "            'junctions',\n",
    "            'storage',\n",
    "            # 'properties',\n",
    "            # \"curves\",\n",
    "            ]\n",
    "\n",
    "RELEVANT_LINKS = [\n",
    "            # 'channel10944',\n",
    "            # 'channel24416',\n",
    "            # 'channel60443',\n",
    "            # 'channel17459',\n",
    "            # 'channel87859',\n",
    "            # 'channel14380',\n",
    "            # 'channel55414',\n",
    "            # 'channel77496',\n",
    "            # 'channel83013',\n",
    "            # 'channel52767',\n",
    "            # 'channel12818',\n",
    "            # 'conduit11698',\n",
    "            # 'channel6317',\n",
    "            # 'conduit18801',\n",
    "            # 'conduit50317',\n",
    "            # 'conduit528',\n",
    "            # 'conduit36611',\n",
    "            # 'conduit50827',\n",
    "            # 'conduit78108',\n",
    "            # 'conduit57848',\n",
    "            # 'conduit42638',\n",
    "            # 'conduit34157',\n",
    "            # 'conduit29340',\n",
    "            # 'conduit19715',\n",
    "            # 'conduit23023',\n",
    "            # 'conduit37130',\n",
    "            # 'conduit21772',\n",
    "            # 'channel52598',\n",
    "            # 'conduit75783',\n",
    "            # 'conduit62715',\n",
    "            # 'conduit48979',\n",
    "            # 'conduit82544',\n",
    "            # 'conduit83110',\n",
    "            # 'conduit33678',\n",
    "            # 'conduit18303',\n",
    "            # 'conduit40724',\n",
    "            # 'conduit13927'\n",
    "        ]\n",
    "\n",
    "RELEVANT_SUBCATCHMENTS = []\n",
    "\n",
    "RELEVANT_NODES = []\n",
    "\n",
    "RELEVANT_SUBAREAS = []\n",
    "\n",
    "RELEVANT_OUTFALLS = []\n",
    "\n",
    "RELEVANT_VERTICES = []\n",
    "\n",
    "RELEVANT_POLYGNOS = []\n",
    "\n",
    "RELEVANT_LINKS_CONDUITS = []\n",
    "\n",
    "RELEVANT_LINKS_ORIFICES = []\n",
    "\n",
    "RELEVANT_LINKS_WEIRS = []\n",
    "\n",
    "RELEVANT_LOSSES = []\n",
    "\n",
    "RELEVANT_INFILTRATION = []\n",
    "\n",
    "RELEVANT_JUNCTIONS = []\n",
    "\n",
    "RELEVANT_STORAGE = []\n",
    "\n",
    "\n",
    "\n",
    "MODEL_OUT_COLS = {\n",
    "    'SUBCATCHMENTS_COLS' : [\n",
    "        'event_id',\n",
    "        'elapsed_time',\n",
    "        'subcatchment_id',\n",
    "        'rainfall',\n",
    "        'elapsed_time',\n",
    "        'snow_depth',\n",
    "        'evaporation_loss',\n",
    "        'infiltration_loss',\n",
    "        'runoff_rate',\n",
    "        'groundwater_outflow',\n",
    "        'groundwater_elevation',\n",
    "        'soil_moisture'\n",
    "    ],\n",
    "    'LINKS_COLS' : [\n",
    "        'event_id',\n",
    "        'elapsed_time',\n",
    "        'link_id',\n",
    "        'flow_rate',\n",
    "        'flow_depth',\n",
    "        'flow_velocity',\n",
    "        'froude_number',\n",
    "        'capacity'\n",
    "    ],\n",
    "    'NODES_COLS' : [\n",
    "        'event_id',\n",
    "        'elapsed_time',\n",
    "        'node_id',\n",
    "        'depth_above_invert',\n",
    "        'hydraulic_head',\n",
    "        'volume_stored_ponded',\n",
    "        'lateral_inflow',\n",
    "        'total_inflow',\n",
    "        'flow_lost_flooding'\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "MODEL_INP_COLS = {\n",
    "    'NODES_COORDINATES' : [\n",
    "        'node_id',\n",
    "        'x_coord',\n",
    "        'y_coord',\n",
    "    ],\n",
    "    \"SUBCATCHMENTS\" : [\n",
    "      \"subcatchment_id\",\n",
    "      \"raingage_id\",\n",
    "      \"outlet\",\n",
    "      \"area\",\n",
    "      \"imperv\",\n",
    "      \"width\",\n",
    "      \"slope\",\n",
    "      \"curb_len\"\n",
    "    ],\n",
    "    \"LINKS_CONDUITS\" : [\n",
    "         \"conduit_id\",\n",
    "         \"from_node\",\n",
    "         \"to_node\",\n",
    "         \"length\",\n",
    "         \"roughness\",\n",
    "         \"in_offset\",\n",
    "         \"out_offset\",\n",
    "         \"init_flow\",\n",
    "         \"max_flow\"\n",
    "    ],\n",
    "    \"LINKS_ORIFICES\" : [\n",
    "          \"orifice_id\",\n",
    "          \"from_node\",\n",
    "          \"to_node\",\n",
    "          \"type\",\n",
    "          \"offset\",\n",
    "          \"q_coeff\",\n",
    "          \"gated\",\n",
    "          \"close_time\"\n",
    "    ],\n",
    "    \"LINKS_WEIRS\" : [\n",
    "          \"weir_id\",\n",
    "          \"from_node\",\n",
    "          \"to_node\",\n",
    "          \"type\",\n",
    "          \"crest_ht\",\n",
    "          \"q_coeff\",\n",
    "          \"gated\",\n",
    "          \"end_con\",\n",
    "          \"end_coeff\",\n",
    "          \"surcharge\"\n",
    "    ],\n",
    "    \"SUBAREAS\" : [\n",
    "        \"subcatchment_id\",\n",
    "        \"n_imperv\",\n",
    "        \"n_perv\",\n",
    "        \"s_imperv\",\n",
    "        \"s_perv\",\n",
    "        \"pct_zero\",\n",
    "        \"route_to\"\n",
    "    ],\n",
    "    \"NODES_STORAGE\" : [\n",
    "        \"storage_id\",\n",
    "        \"elevation\",\n",
    "        \"max_depth\",\n",
    "        \"init_depth\",\n",
    "        \"shape\",\n",
    "        \"curve_name_params\",\n",
    "        \"n_a\",\n",
    "        \"f_evap\"\n",
    "    ],\n",
    "    \"NODES_OUTFALLS\" : [\n",
    "        \"outfall_id\",\n",
    "        \"elevation\",\n",
    "        \"type\",\n",
    "        # \"stage_data\",\n",
    "        \"gated\",\n",
    "        # \"route_to\"\n",
    "    ],\n",
    "    \"NODES_JUNCTIONS\" : [\n",
    "        \"junction_id\",\n",
    "        \"elevation\",\n",
    "        \"max_depth\",\n",
    "        \"init_depth\",\n",
    "        \"sur_depth\",\n",
    "        \"aponded\"\n",
    "    ],\n",
    "    \"INFILTRATION\": [\n",
    "        \"subcatchment_id\",\n",
    "        \"max_rate\",\n",
    "        \"min_rate\",\n",
    "        \"decay\",\n",
    "        \"dry_time\",\n",
    "        \"max_infil\",\n",
    "    ],\n",
    "    # \"POLYGONS\": [\n",
    "    #     \"subcatchment_id\",\n",
    "    #     \"x_coord\",\n",
    "    #     \"y_coord\"\n",
    "    # ],\n",
    "    # \"VERICES\": [\n",
    "    #     \"link_id\",\n",
    "    #     \"x_coord\",\n",
    "    #     \"y_coord\"\n",
    "    # ],\n",
    "    \"PROPERTIES\": [\n",
    "        \"model_name\",\n",
    "        \"model_version\",\n",
    "        \"flow_units\",\n",
    "        \"infiltration\",\n",
    "        \"flow_routing\",\n",
    "        \"link_offsets\",\n",
    "        \"min_slope\",\n",
    "        \"allow_ponding\",\n",
    "        \"skip_steady_state\",\n",
    "        \"start_date\",\n",
    "        \"start_time\",\n",
    "        \"report_start_date\",\n",
    "        \"report_start_time\",\n",
    "        \"end_date\",\n",
    "        \"end_time\",\n",
    "        \"sweep_start\",\n",
    "        \"sweep_end\",\n",
    "        \"report_step\",\n",
    "        \"wet_step\",\n",
    "        \"dry_step\",\n",
    "        \"routing_step\",\n",
    "        \"inertial_damping\",\n",
    "        \"normal_flow_limited\",\n",
    "        \"force_main_equation\",\n",
    "        \"variable_step\",\n",
    "        \"lengthening_step\",\n",
    "        \"min_surfarea\",\n",
    "        \"max_trials\",\n",
    "        \"head_tolerance\",\n",
    "        \"sys_flow\",\n",
    "        \"lat_flow_tol\",\n",
    "        \"minimum_step\",\n",
    "        \"threads\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# dictionary to store data\n",
    "groups = {}\n",
    "\n",
    "\n",
    "# Definition of starting postiion of each element\n",
    "def group_start_line(model):\n",
    "    with open(model, 'r') as inp:\n",
    "        groups = {}\n",
    "        count = 0\n",
    "\n",
    "        lines = inp.readlines()\n",
    "        for line in lines:\n",
    "            if ('[' in line) & (']' in line):\n",
    "                groups.update({line[1:-2].lower() : {'start': count}})\n",
    "            count += 1\n",
    "    # subselection of elements from MODEL_ELEMENTS\n",
    "    groups = {key:value for key,value in groups.items() if key in RELEVANT_GROUP_TYPES_INP}\n",
    "\n",
    "    LINK_TYPES = ['orifices', 'conduits', 'weirs']\n",
    "    NODE_TYPES = ['outfalls', 'junctions', 'storage']\n",
    "\n",
    "    for key in [key for key in groups.keys() if key in LINK_TYPES]:\n",
    "        groups['links_' + key] = groups.pop(key)\n",
    "\n",
    "    for key in [key for key in groups.keys() if key in NODE_TYPES]:\n",
    "        groups['nodes_' + key] = groups.pop(key)\n",
    "\n",
    "    groups['nodes_coordinates'] = groups.pop('coordinates')\n",
    "\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "\n",
    "# adding header and skip-lines to elements dict\n",
    "def build_groups_dicts(model):\n",
    "    groups = group_start_line(model)\n",
    "    count = 0\n",
    "\n",
    "    for element, start_dict in groups.items():\n",
    "        start = start_dict['start']\n",
    "\n",
    "        with open(model, 'r') as inp:\n",
    "            lines = inp.readlines()\n",
    "            for index, line in enumerate(lines):\n",
    "                if (index - start == 1) & (';;' in line) & (';;--' not in line):\n",
    "                    groups[element].update({'header':[col for col in re.split(\"\\s\\s+\", line[2:-1]) if len(col) > 1]})\n",
    "\n",
    "                elif (index - start == 2) & (';;--------------' in line):\n",
    "                    groups[element].update({'line_to_skip': index})\n",
    "\n",
    "                elif (index - start == 3):\n",
    "                    break\n",
    "\n",
    "\n",
    "    # some corrrections on header because of mismatches on inp file\n",
    "\n",
    "    # groups['properties'].update({'header': MODEL_INP_COLS['PROPERTIES']})\n",
    "    groups['subcatchments'].update({'header': MODEL_INP_COLS['SUBCATCHMENTS']})\n",
    "    groups['subareas'].update({'header': MODEL_INP_COLS['SUBAREAS']})\n",
    "    groups['infiltration'].update({'header': MODEL_INP_COLS['INFILTRATION']})\n",
    "    groups['links_conduits'].update({'header': MODEL_INP_COLS['LINKS_CONDUITS']})\n",
    "    groups['links_weirs'].update({'header': MODEL_INP_COLS['LINKS_WEIRS']})\n",
    "    groups['links_orifices'].update({'header': MODEL_INP_COLS['LINKS_ORIFICES']})\n",
    "    groups['nodes_coordinates'].update({'header': MODEL_INP_COLS['NODES_COORDINATES']})\n",
    "    groups['nodes_outfalls'].update({'header': MODEL_INP_COLS['NODES_OUTFALLS']})\n",
    "    groups['nodes_storage'].update({'header': MODEL_INP_COLS['NODES_STORAGE']})\n",
    "    groups['nodes_junctions'].update({'header': MODEL_INP_COLS['NODES_JUNCTIONS']})\n",
    "\n",
    "    return groups\n",
    "\n",
    "# %%\n",
    "\n",
    "def list_files(directory, extension, prefix):\n",
    "    return (f for f in listdir(directory) if (f.endswith('.' + extension)) & (f.startswith(prefix)))\n",
    "\n",
    "\n",
    "def raingages_meta_to_dfs(model, model_id):\n",
    "    \"\"\" Read a .CSV into a Pandas DataFrame until a blank line is found, then stop.\n",
    "    \"\"\"\n",
    "    start = build_groups_dicts(model)['raingages']['start']\n",
    "    skip_rows = build_groups_dicts(model)['raingages']['line_to_skip']\n",
    "    header = ['raingage_id', 'format', 'interval',  'unit']\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    with open(model, newline='') as f:\n",
    "        contents = []\n",
    "        r = csv.reader(f)\n",
    "        for i, line in enumerate(r):\n",
    "            if i >  start + 1:\n",
    "                if i != skip_rows:\n",
    "                    if not line:\n",
    "                        break\n",
    "                    # elif i == start + 1:\n",
    "                    #     headers = line\n",
    "                    else:\n",
    "                        formatted_line = [line[0].split()[0], line[0].split()[1], line[0].split()[2],line[0].split()[7]]\n",
    "                        contents.append(formatted_line)\n",
    "\n",
    "    df = pd.DataFrame(data = contents, columns= header,)\n",
    "    df['interval'] = df['interval'].map( lambda x: datetime.strptime(x, '%H:%M'))\n",
    "    df.insert(0, 'precipitation_id', precipitation_id)\n",
    "    print('raingages','df created!')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def date_parser(line):\n",
    "    year = line[0].split()[1]\n",
    "    month = line[0].split()[2].zfill(2)\n",
    "    day = line[0].split()[3].zfill(2)\n",
    "    hour = line[0].split()[4].zfill(2)\n",
    "    minute = line[0].split()[5].zfill(2)\n",
    "\n",
    "    str_date = '-'.join([year, month, day, hour, minute] )\n",
    "\n",
    "    date_format = '%Y-%m-%d-%H-%M'\n",
    "    return datetime.strptime(str_date, date_format)\n",
    "\n",
    "# %%\n",
    "def raingages_to_df(event_folder, event_id, model, model_id):\n",
    "    contents = []\n",
    "\n",
    "    for file in list_files(event_folder, 'txt', 'P'):\n",
    "        raingage_id = file.split('.')[0]\n",
    "\n",
    "        with open(os.path.join(event_folder, file), newline='') as f:\n",
    "            r = csv.reader(f)\n",
    "            for i, line in enumerate(r):\n",
    "                try:\n",
    "                    formatted_line = [\n",
    "                        raingage_id,\n",
    "                        date_parser(line),\n",
    "                        line[0].split()[6]\n",
    "                        ]\n",
    "                    contents.append(formatted_line)\n",
    "                except:\n",
    "                    print('error')\n",
    "    df_timeseries = pd.DataFrame(data = contents, columns= ['raingage_id', 'elapsed_time', 'value'])\n",
    "    df_timeseries.insert(0, 'precipitation_id', precipitation_id)\n",
    "\n",
    "    df_metadata = raingages_meta_to_dfs(model, model_id)\n",
    "    return df_metadata, df_timeseries\n",
    "# %%\n",
    "\n",
    "def load_raingages_to_db(event_folder, event_id, model, model_id):\n",
    "    raingage_metadata, raingage_timeseries = raingages_to_df(event_folder, event_id, model, model_id)\n",
    "\n",
    "    table_metadata = 'raingages_metadata'\n",
    "    table_timeseries = 'raingages_timeseries'\n",
    "\n",
    "    try:\n",
    "        raingage_metadata.to_sql(table_metadata, engine_base_ina, index=False, if_exists='append')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    try:\n",
    "        raingage_timeseries.to_sql(table_timeseries, engine_base_ina, index=False, if_exists='append')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# def group_type_to_dfs(model, model_id, group, id_col, col_to_check, own_relevant__list, relevant_dependent_list):\n",
    "#     \"\"\" Read a .CSV into a Pandas DataFrame until a blank line is found, then stop.\n",
    "#     \"\"\"\n",
    "#     start = build_groups_dicts(model)[group]['start']\n",
    "#     skip_rows = build_groups_dicts(model)[group]['line_to_skip']\n",
    "#     header = build_groups_dicts(model)[group]['header']\n",
    "\n",
    "#     global own_relevant__list\n",
    "#     own_relevant_list = []\n",
    "\n",
    "#     df = pd.DataFrame()\n",
    "#     with open(model, newline='') as f:\n",
    "#         contents = []\n",
    "#         r = csv.reader(f)\n",
    "#         for i, line in enumerate(r):\n",
    "#             if i >=  start + 1:\n",
    "#                 if i != skip_rows:\n",
    "#                     if not line:\n",
    "#                         break\n",
    "#                     # elif i == start + 1:\n",
    "#                     #     headers = line\n",
    "#                     else:\n",
    "#                         if len(relevant_dependecy_list) == 0:\n",
    "#                             own_relevant__list.append(line[0].split()[id_col])\n",
    "#                             contents.append(line[0].split())\n",
    "#                         else:\n",
    "#                             if line[0].split()[col_to_check].lower() in relevant_dependent_list:\n",
    "#                                 own_relevant__list.append(line[0].split()[id_col])\n",
    "#                                 contents.append(line[0].split())\n",
    "\n",
    "#     df = pd.DataFrame(data = contents, columns= [col.lower().replace(\"-\", \"_\").replace(\"%\", \"\").replace(\" \", \"_\") for col in header],)\n",
    "#     df.insert(0, 'model_id', model_id)\n",
    "#     print(group,'df created!')\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def conduits_to_dfs(model, model_id):\n",
    "    \"\"\" Read a .CSV into a Pandas DataFrame until a blank line is found, then stop.\n",
    "    \"\"\"\n",
    "    start = build_groups_dicts(model)['links_conduits']['start']\n",
    "    skip_rows = build_groups_dicts(model)['links_conduits']['line_to_skip']\n",
    "    header = build_groups_dicts(model)['links_conduits']['header']\n",
    "\n",
    "    global RELEVANT_LINKS_CONDUITS\n",
    "    RELEVANT_LINKS_CONDUITS = []\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    with open(model, newline='') as f:\n",
    "        contents = []\n",
    "        r = csv.reader(f)\n",
    "        for i, line in enumerate(r):\n",
    "            if i >  start + 1:\n",
    "                if i != skip_rows:\n",
    "                    if not line:\n",
    "                        break\n",
    "                    # elif i == start + 1:\n",
    "                    #     headers = line\n",
    "                    else:\n",
    "                        if len(RELEVANT_LINKS) == 0:\n",
    "                            contents.append(line[0].split())\n",
    "                        else:\n",
    "                            if line[0].split()[0].lower() in RELEVANT_LINKS:\n",
    "                                RELEVANT_LINKS_CONDUITS.append(line[0].split()[0])\n",
    "                                contents.append(line[0].split())\n",
    "\n",
    "    df = pd.DataFrame(data = contents, columns= [col.lower().replace(\"-\", \"_\").replace(\"%\", \"\").replace(\" \", \"_\") for col in header],)\n",
    "    df.insert(0, 'model_id', model_id)\n",
    "    print('conduits','df created!')\n",
    "    return df\n",
    "\n",
    "def weirs_to_dfs(model, model_id):\n",
    "    \"\"\" Read a .CSV into a Pandas DataFrame until a blank line is found, then stop.\n",
    "    \"\"\"\n",
    "    start = build_groups_dicts(model)['links_weirs']['start']\n",
    "    skip_rows = build_groups_dicts(model)['links_weirs']['line_to_skip']\n",
    "    header = build_groups_dicts(model)['links_weirs']['header']\n",
    "\n",
    "    global RELEVANT_LINKS_WEIRS\n",
    "    RELEVANT_LINKS_WEIRS = []\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    with open(model, newline='') as f:\n",
    "        contents = []\n",
    "        r = csv.reader(f)\n",
    "        for i, line in enumerate(r):\n",
    "            if i >  start + 1:\n",
    "                if i != skip_rows:\n",
    "                    if not line:\n",
    "                        break\n",
    "                    # elif i == start + 1:\n",
    "                    #     headers = line\n",
    "                    else:\n",
    "                        if len(RELEVANT_LINKS) == 0:\n",
    "                            contents.append(line[0].split())\n",
    "                        else:\n",
    "                            if line[0].split()[0].lower() in RELEVANT_LINKS:\n",
    "                                RELEVANT_LINKS_WEIRS.append(line[0].split()[0])\n",
    "                                contents.append(line[0].split())\n",
    "\n",
    "    df = pd.DataFrame(data = contents, columns= [col.lower().replace(\"-\", \"_\").replace(\"%\", \"\").replace(\" \", \"_\") for col in header],)\n",
    "    df.insert(0, 'model_id', model_id)\n",
    "    print('weirs','df created!')\n",
    "    return df\n",
    "\n",
    "\n",
    "def orifices_to_dfs(model, model_id):\n",
    "    \"\"\" Read a .CSV into a Pandas DataFrame until a blank line is found, then stop.\n",
    "    \"\"\"\n",
    "    start = build_groups_dicts(model)['links_orifices']['start']\n",
    "    skip_rows = build_groups_dicts(model)['links_orifices']['line_to_skip']\n",
    "    header = build_groups_dicts(model)['links_orifices']['header']\n",
    "\n",
    "    global RELEVANT_LINKS_ORIFICES\n",
    "    RELEVANT_LINKS_ORIFICES = []\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    with open(model, newline='') as f:\n",
    "        contents = []\n",
    "        r = csv.reader(f)\n",
    "        for i, line in enumerate(r):\n",
    "            if i >  start + 1:\n",
    "                if i != skip_rows:\n",
    "                    if not line:\n",
    "                        break\n",
    "                    # elif i == start + 1:\n",
    "                    #     headers = line\n",
    "                    else:\n",
    "                        if len(RELEVANT_LINKS) == 0:\n",
    "                            contents.append(line[0].split())\n",
    "                        else:\n",
    "                            if line[0].split()[0].lower() in RELEVANT_LINKS:\n",
    "                                RELEVANT_LINKS_ORIFICES.append(line[0].split()[0])\n",
    "                                contents.append(line[0].split())\n",
    "\n",
    "    df = pd.DataFrame(data = contents, columns= [col.lower().replace(\"-\", \"_\").replace(\"%\", \"\").replace(\" \", \"_\") for col in header],)\n",
    "    df.insert(0, 'model_id', model_id)\n",
    "    print('orifices','df created!')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_nodes_from_links(model, model_id):\n",
    "    conduits_df = conduits_to_dfs(model, model_id)\n",
    "    orifices_df = orifices_to_dfs(model, model_id)\n",
    "    weirs_df = weirs_to_dfs(model, model_id)\n",
    "\n",
    "    links_dfs = [\n",
    "        conduits_df,\n",
    "        orifices_df,\n",
    "        weirs_df\n",
    "    ]\n",
    "\n",
    "    nodes = []\n",
    "    for df in links_dfs:\n",
    "        for col in [col for col in df.columns if 'node' in col]:\n",
    "            nodes += df[col].unique().tolist()\n",
    "\n",
    "    return nodes\n",
    "\n",
    "#cambio de coordenadas\n",
    "def convert_coords(coord_tuple):\n",
    "    transformer = pyproj.Transformer.from_crs(crs_from='epsg:' + epsg_modelo, crs_to='epsg:4326')\n",
    "    lon, lat = transformer.transform(coord_tuple[0], coord_tuple[1])\n",
    "    return (lon,lat)\n",
    "\n",
    "def nodes_to_dfs(model, model_id):\n",
    "    \"\"\" Read a .CSV into a Pandas DataFrame until a blank line is found, then stop.\n",
    "    \"\"\"\n",
    "    global RELEVANT_NODES\n",
    "    RELEVANT_NODES = get_nodes_from_links(model, model_id)\n",
    "\n",
    "    start = build_groups_dicts(model)['nodes_coordinates']['start']\n",
    "    skip_rows = build_groups_dicts(model)['nodes_coordinates']['line_to_skip']\n",
    "    header = build_groups_dicts(model)['nodes_coordinates']['header']\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    with open(model, newline='') as f:\n",
    "        contents = []\n",
    "        r = csv.reader(f)\n",
    "        for i, line in enumerate(r):\n",
    "            if i >  start + 1:\n",
    "                if i != skip_rows:\n",
    "                    if not line:\n",
    "                        break\n",
    "                    # elif (i == start + 1):\n",
    "                    #     headers = line\n",
    "\n",
    "                    else:\n",
    "                        if len(RELEVANT_NODES) == 0:\n",
    "                            contents.append(line[0].split())\n",
    "                        else:\n",
    "                            if line[0].split()[0] in RELEVANT_NODES:\n",
    "                                contents.append(line[0].split())\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data = contents, columns= [col.lower().replace(\"-\", \"_\").replace(\"%\", \"\").replace(\" \", \"_\") for col in header],)\n",
    "    df.insert(0, 'model_id', model_id)\n",
    "\n",
    "    # cols =['lat', 'lon']\n",
    "    # coords = []\n",
    "\n",
    "    # # for i in df[['x_coord', 'y_coord']].iterrows():\n",
    "    # #     coords.append(convert_coords(i[1]))\n",
    "\n",
    "\n",
    "    # from pyproj import Transformer\n",
    "\n",
    "    # def convert_coords(coord_tuple):\n",
    "    #     nonlocal coords\n",
    "    #     transformer = Transformer.from_crs(crs_from='epsg:5348' , crs_to='epsg:4326')\n",
    "    #     lon, lat = transformer.transform(coord_tuple[0], coord_tuple[1])\n",
    "    #     coords.append((lon, lat))# coord_tuple[2]))\n",
    "\n",
    "    #     # return coords\n",
    "\n",
    "\n",
    "    # import concurrent.futures\n",
    "\n",
    "\n",
    "    # with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    #     executor.map(convert_coords, [(j[0], j[1]) for i,j in df[['x_coord', 'y_coord']].iterrows()])\n",
    "\n",
    "    # # coords = result[-1]\n",
    "\n",
    "    # df = pd.concat([df, pd.DataFrame(coords, columns=cols)], axis=1)\n",
    "\n",
    "    # print('nodes','df created!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "conduits df created!\n",
      "orifices df created!\n",
      "weirs df created!\n"
     ]
    }
   ],
   "source": [
    "df_a = nodes_to_dfs(model_inp, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = []\n",
    "def convert_coords(coord_tuple):\n",
    "    global coords\n",
    "    transformer = Transformer.from_crs(crs_from='epsg:5348' , crs_to='epsg:4326')\n",
    "    lon, lat = transformer.transform(coord_tuple[0], coord_tuple[1])\n",
    "    \n",
    "    coords.append((lon, lat))# coord_tuple[2]))\n",
    "\n",
    "    # return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c0b4829e5fa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcessPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/concurrent/futures/process.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue_management_thread_wakeup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue_management_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0;31m# To reduce the risk of opening too many files, remove references to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;31m# objects that use file descriptors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "from pyproj import Transformer\n",
    "\n",
    "coordinates = [(j[0], j[1]) for i,j in df_a[['x_coord', 'y_coord']].iterrows()]\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    executor.map(convert_coords, coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "len(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}